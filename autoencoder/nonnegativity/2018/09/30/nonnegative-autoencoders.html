<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Nonnegative Autoencoders | Robots, machine learning, global issues</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Nonnegative Autoencoders" />
<meta name="author" content="Anne van Rossum" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Autoencoders with a nonnegative constraint that encourages part-based representations at the latent layer." />
<meta property="og:description" content="Autoencoders with a nonnegative constraint that encourages part-based representations at the latent layer." />
<link rel="canonical" href="https://annevanrossum.com/autoencoder/nonnegativity/2018/09/30/nonnegative-autoencoders.html" />
<meta property="og:url" content="https://annevanrossum.com/autoencoder/nonnegativity/2018/09/30/nonnegative-autoencoders.html" />
<meta property="og:site_name" content="Robots, machine learning, global issues" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-09-30T14:36:27+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Nonnegative Autoencoders" />
<script type="application/ld+json">
{"description":"Autoencoders with a nonnegative constraint that encourages part-based representations at the latent layer.","@type":"BlogPosting","headline":"Nonnegative Autoencoders","dateModified":"2018-09-30T14:36:27+00:00","datePublished":"2018-09-30T14:36:27+00:00","url":"https://annevanrossum.com/autoencoder/nonnegativity/2018/09/30/nonnegative-autoencoders.html","author":{"@type":"Person","name":"Anne van Rossum"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://annevanrossum.com/autoencoder/nonnegativity/2018/09/30/nonnegative-autoencoders.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://annevanrossum.com/feed.xml" title="Robots, machine learning, global issues" />

  
  

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
  <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
  <script>
      document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
          ]}
        );
      });
    </script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Robots, machine learning, global issues</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Nonnegative Autoencoders</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-09-30T14:36:27+00:00" itemprop="datePublished">Sep 30, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>My intuition would say that a part-based decomposition should arise naturally within an autoencoder. To encorporate
the next image in an image recognition task, it must be more beneficial to have gradient descent being able to 
navigate towards the optimal set of neural network weights for that image. If not, for each image gradient descent
is all the time navigating some kind of common denominator, none of the images are actually properly represented.
For each new image that is getting better classified, the other images are classified worse. With a proper
decomposition learning the next representation will not interfere with previous representations. Grossberg calls this
in Adaptive Resonance Theory (ART) catastrophic forgetting.</p>

<p>Maybe if we train a network long enough this will be the emerging decomposition strategy indeed. However, this is not
what is normally found. The different representations get coupled and there is not a decomposition that allows
the network to explore different feature dimensions independently.</p>

<p>One of the means to obtain a part-based representation is to force positive or zero weights in a network. In the 
literature <a href="https://yliapis.github.io/Non-Negative-Matrix-Factorization/">nonnegative matrix factorization</a> can be 
found. Due to the nonnegativity constraint the features are additive. This leads to a (sparse) basis where through
summation “parts” are added up to a “whole” object. For example, faces are built up out of features like eyes, nostrils,
mouth, ears, eyebrows, etc.</p>

<p><img src="/images/blog/nonnegative_examples.jpg" alt="Nonnegative examples. From top to bottom: 1) Sparse Autoencoder, 2) Nonnegative Sparse Autoencoder, 3) Nonnegativity Constrained Autoencoder, and 4) Nonnegative Matrix Factorization. The nonnegative examples do not use clear cut facial features like eyes and ears, but you see only parts of the image being nonnegative (white). This means an image can be composed using a sum of the displayed images. Copyright Hosseini-Asl et al." /></p>

<!--more-->

<h1 id="sparse-autoencoder-with-nonnegativity-constraint">Sparse Autoencoder with Nonnegativity Constraint</h1>

<p>At Louisville university
<a href="https://github.com/ehosseiniasl">Ehsan Hosseini-Asl (github)</a>,
<a href="http://www.jacekzurada.org/">Jacek Zurada</a> (who is running for 2019 IEEE president), and
<a href="https://twitter.com/olfanasraoui">Olfa Nasraoui (twitter)</a>
studied how nonnegative constraints can be added to an autoencoder in 
<a href="https://arxiv.org/pdf/1601.02733.pdf">Deep Learning of Part-based Representation of Data Using Sparse Autoencoders with Nonnegativity Constraints (2016)</a>.</p>

<p>An autoencoder which has a latent layer that contains a part-based representation, only has a few of the nodes active
at a particular input. In other words, such a representation is sparse.</p>

<p>One of the ways a sparse representation can be enforced is to limit the activation of a hidden unit over all data
items \(r\). The average activation of a unit is:</p>

\[\hat{p}_j = \frac{1}{m} \sum_{r=1}^m h_j(x^{(r)})\]

<p>To make sure that the activation is limited, we can bound \(\hat{p}_j &lt; p\) with \(p\) a small value close to zero.</p>

<p>The usual cost function is just the reconstruction error \(J_E\). Here, we include the activation limitation by adding an additional term:</p>

\[J_{KL}(p || \hat{p})  = \sum_{j=1}^n p \log \frac{p}{\hat{p}_j} + (1-p) \log \frac{1-p}{1-\hat{p}_j}\]

<p>We can prevent overfitting by regularization. This can be done by adding noise to the input, dropout, or by penalizing large weights. The latter corresponds to yet another term:</p>

\[J_{O}(W,b) = \frac{1}{2} \sum_{l=1}^2 \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} \left( w_{ij}^l \right)^2\]

<p>The sizes of adjacent layers are indicated by \(s_l\) and \(s_{l+1}\) (and we are limited to \(l\) layers).</p>

<p>The total cost function used by the authors for the sparse autoencoder contains all the above cost functions, each weighted, one by parameter \(\beta\), the other by \(\lambda\).</p>

\[J_{SAE}(W,b) = J_E(W,b) + \beta J_{KL}(p||\hat{p}) + \lambda J_O(W,b)\]

<p>To enforce nonnegativity we can introduce a different regularization term:</p>

\[J_{O}(W,b) = \frac{1}{2} \sum_{l=1}^2 \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} f \left( w_{ij}^l \right)\]

<p>For the nonnegative constrained autoencoder the authors suggest:</p>

\[f(w_{ij}) = 
\begin{cases}
w_{ij}^2 &amp; w_{ij} &lt; 0 \\ 
0 &amp; \text{otherwise}
\end{cases}\]

<p>This term penalizes all negative values. All positive values do not contribute to the cost function.</p>

<h2 id="results">Results</h2>

<p>Results are compared between the Sparse Autoencoder (SAE), the Nonnegative Sparse Autoencoder (NNSAE), the Negatively 
Constrained Autoencoder (NCAE), and Nonnegative Matrix Factorization (NMF).</p>

<p><img src="/images/blog/nonnegative_autoencoder_representation_comparison.jpg" alt="Comparison of representations. 1) SAE, 2) NNSAE, 3) NCAE, 4) NMF" /></p>

<p>The SAE representation contains negative values (dark pixels). The NNSAE representation has neurons with zero weights (complete black nodes).</p>

<p>The receptive fields learned by NCAE are more sparse than the others. 
The features from NNSAE and NMF are more local.</p>

<p><img src="/images/blog/nonnegative_constrained_mnist_comparison.png" alt="Nonnegative Constrained Autoencoder compared using the MNIST classification task with other reconstruction methods. Rows: 1) Original digits, 2) Sparse Autoencoder, 3) Nonnegative Sparse Autoencoder, 4) Negatively Constrained Autoencoder, and Nonnegative Matrix Factorization." /></p>

<h2 id="ideas">Ideas</h2>

<p>To really encourage a part-based decomposition it would be best to enforce either very large values or values that are zero. Something like sum over x divided by number of non-zero components with each x nonnegative and maximizing over this.</p>


  </div><a class="u-url" href="/autoencoder/nonnegativity/2018/09/30/nonnegative-autoencoders.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Robots, machine learning, global issues</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Anne van Rossum</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/mrquincle"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">mrquincle</span></a></li><li><a href="https://www.twitter.com/annevanrossum"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">annevanrossum</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog about robots, machine learning, and other random stuff</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
