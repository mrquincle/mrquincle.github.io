---
layout: post
title: "Generating Point Clouds"
description: "Points in a 3D space can be generated by auto-encoders. In this way the deep network learns representations that are intrinsic to point clouds."
date: 2018-09-26 10:15:22 +0200
comments: true
categories: [deep learning, autoencoders, point clouds]
facebook:
  image: /images/blog/dfaust_dataset.png
twitter_card:
  type: summary_large_image
  image: /images/blog/dfaust_dataset.png
---

If we do want robots to learn about the world, we can use computer vision. We can employ traditional methods. Build up a full-fledged model from corner detectors, edge detectors, feature descriptors, gradient descriptors, etc. We can also use modern deep learning techniques. One large neural network hopefully captures similarly or even better abstractions compared to the conventional computer vision pipeline. 

Computer vision is not the only choice though! In recent years there is a proliferation of a different type of data: depth data. Collections (or clouds) of points represent 3D shapes. In a game setting the Kinect was a world-shocking invention using structured light. In robotics and autonomous cars LIDARs are used. There is huge debate about which sensors are gonna "win", but I personally doubt there will be a clearcut winner. My line of reasoning:

* Humans use vision and have perfected this in many ways. It would be silly to not use cameras.
* Depth sensors can provide information when vision gets obstructed.
* Humans use glasses, microscopes, infrared goggles, all to enhance our senses. We are basically cyborgs.
* Robots will benefit from a rich sensory experience just like we do. They want to be cyborgs too.

# Point clouds

Point clouds are quite a good description. Objects are represented by individual points in a 3D space. By making the points a bit bigger you can easily figure out the shapes yourself (see the figure below).

![The D-FAUST dataset and interpolation between the figure totally at the left and the one totally at the right. Copyright: Achlioptas et al. (2018).](/images/blog/dfaust_dataset.png "Interpolation between two distinct human figures shows a gradual transition from one figure to the next")

A group of researchers who started to study point clouds are 
[Panos Achlioptas](http://web.stanford.edu/~optas/),
[Olga Diamanti](https://geometry.stanford.edu/person.php?id=diamanti),
[Ioannis Mitliagkas](http://mitliagkas.github.io/), and
[Leonidas Guibas](https://geometry.stanford.edu/member/guibas/) in the paper [Learning Representations and Generative Models for 3D Point Clouds](https://arxiv.org/pdf/1707.02392.pdf) from Leonidas Guibas' Geometric Computation group in the Computer Science Department of Stanford University. Side note: prof. Guibas obtained his PhD under the supervision of Donald Knuth. You can reach Panos via [his website](http://web.stanford.edu/~optas/contact.html). I haven't found his Twitter account.

There are two main reasons why point clouds should be treated different from pixels in a neural network:

1. The convolution operator works on a grid. Encoding 3D data on a grid would encode a lot of empty voxels. This means that for point clouds we cannot just convolution.
2. Point clouds are permutation invariant. Only the 3D position of a point matters, their id does not. Points in a point cloud can be numbered in any way. It still describes the same object. A comparison operator between two point clouds need to take this into account. Preferably the latent variables will also be permutation invariant.

## Permutation invariant distance metrics

There are permutation invariant distance metrics available. The authors describe the [Earth Mover's Distance (EM)](https://en.wikipedia.org/wiki/Earth_mover%27s_distance). This is a concept discovered by Gaspard Monge in 1781 on how to transport soil from one place to the next with minimal effort. Let us define the flow $$f_{i,j}$$ between location $$i$$ and $$j$$ with distance $$d_{i,j}$$, then the EM distance between location $$i \in P$$ and $$j \in Q$$ is as follows:

$$d_{EM}(P,Q) = \frac{ \sum_{i=1}^m \sum_{j=1}^n f_{i,j} d_{i,j} }{ \sum_{i=1}^m \sum_{j=1}^n f_{i,j} }$$

The individual flow is multiplied with the corresponding distance. The overall sum is normalized with the overall flow. In mathematics this is known as the Wasserstein metric. [This blog post](https://vincentherrmann.github.io/blog/wasserstein/) introduces the Wasserstein metric perfectly and [this blog post](https://www.alexirpan.com/2017/02/22/wasserstein-gan.html) explains its relevance to Generative Adversarial Networks.

The Wasserstein metric is differentiable almost everywhere. [Almost everywhere (a.e.)](https://en.wikipedia.org/wiki/Almost_everywhere) is a technical term related to a set having measure zero. It states that the elements for which the property (in this case being differentiable) is not valid has measure zero. Another example of a function that is differentiable a.e. is a monotonic function on $$[a,b] \rightarrow \mathbb{R}$$.

The Chamfer pseudo-distance (C) measure is another measure which calculates the squared distance between each point in one set to the nearest neighbour in the other set:

$$d_{C}(P,Q) = \sum_{i=1}^m \min_j d_{i,j}^2 + \sum_{j=1}^n \min_i d_{i,j}^2$$

It's stated that $$d_{C}$$ is more efficient to compute than $$d_{EM}$$.

Immediately we can observe from these metrics that they are not invariant with respect to rotations, translations, or scaling.

## Comparison metrics

To compare shapes in a 3D space we can follow different strategies. We describe three methods that take the spatial
nature into account and that compare over sets of 3D objects (so we can compare set $$P$$ with set $$Q$$): 

1. Jensen-Shannon divergence.
2. Coverage
3. Minimum Matching distance.

### Jensen-Shannon

First of all, we can align the point cloud data along the axis, introduce voxels and measure the number of points in
each corresponding voxel. Then we use a distance metric using these quantities, in this case a Jensen-Shannon divergence.

$$d_{JS}(P||Q) = \frac{1}{2} D_{KL}(P||M) + \frac{1}{2} D_{KL}(Q||M)$$

Here $$M = \frac{1}{2}(P + Q)$$ and $$D_{KL}$$ is the Kullbach-Leibler divergence metric.

To compare sets of point clouds we can do exactly the same. In this case the number of points in each voxel is just the
collection of points across all point clouds in that set.

### Coverage

Coverage is defined by the fraction of point clouds in $$P$$ that are matched with point clouds in $$Q$$. The match is
defined by one the permutation invariant distance metrics. It's not entirely clear how this is completely specified 
to me. Is there a threshold used that defines if it is a match? Or is it just a sum or average?

### Minimum Matching distance

The minimum matching distance measures also the fidelity of $$P$$ versus $$Q$$ (compared to the coverage metric). This
indeed uses an average over the (permutation invariant) distances between point clouds.

## Generation

Will follow later...


