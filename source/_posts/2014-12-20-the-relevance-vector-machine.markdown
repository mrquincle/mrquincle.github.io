---
layout: post
title: "The relevance vector machine"
date: 2014-12-20 15:14:18 +0100
comments: true
categories: 
---

Support vector machines are a kernel-based method that became quite popular in recent times. A support vector machine (in case of standard use, this is not about [one-class SVMs](https://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/)) is a classifier: it separates data points originating from two different classes. It is smart because - rather than taking every data point seriously - only points that are on the border between two classes are taken into account. The so-called support vectors.

The Bayesian paradigm introduces probability as a measure of the plausibility of a belief. If we postulate that belief can be measured in certain degrees, we might subsequently define proper ways in which belief needs to be update given new evidence. In other words, we have old beliefs (priors), new data (evidence), and new beliefs (posteriors). As soon as you ascribe to the notion that we can talk about probability in this way, Bayes's theorem cannot be circumvented. The Bayesian paradigm isn't a choice, it is the proper way to reason about uncertainties in beliefs.

So consider we have data $\mathbf{x}_i$ and labels $l_i$ in pairs $\(\mathbf{x}_i,l_i\)$ with $i \in \{1 \ldots N\}$ we can consider their relationship as follows:

$$
\begin{equation}
l_i = y(\mathbf{x}_i, \mathbf{w}) + \epsilon_i
\label{eq:regression}
\end{equation}
$$

The variable $\epsilon_i$ denote a noise process, more specific it stems from a Gaussian distribution with a mean that is zero and a variance $\sigma^2$. This relationship between $l_i$ and $\mathbf{x}_i$ can be written down as a probability density function:

$$
p(l_i|\mathbf{x},\mathbf{w},\sigma)=\mathcal{N}(l_i|y(\mathbf{x}_i,\mathbf{w}),\sigma^2)
$$

The zero-mean Gaussian noise plus the function $y$ can just as well be described as only a Gaussian, but now with mean $y$. For each observation we have the same relation where we assume that $l_i$ is entirely described by $\mathbf{x}_i$ and does not require other data points $\mathbf{x}_j$ or other labels $l_j$. 

$$
p(\mathbf{l}|\mathbf{x},\mathbf{w},\sigma)=\prod_{i=0}^{N-1} p(l_i|\mathbf{x},\mathbf{w},\sigma)
$$

Very well, we started with the simple regression problem \eqref{eq:regression} and still we are none the wiser about these $\mathbf{w}$'s popping up. The estimation of a proper noise level $\sigma$ as well as nice values for the parameters $\mathbf{w}$ such that the data is fitted, but not overfitted is what we like to do.

We might introduce well-meaning, but ad-hoc manners that restrict the complexity of the function $f$ or the function $p(l_i|\mathbf{x},\mathbf{w},\sigma)$. 
If we would be able to map any $\mathbf{x}_i$ to any $y_i$ we want, we would not have obtained any relevant insight in what general laws this mapping represents. The margin in support vector machines is one of these ad-hoc ways to reduce complexity. 
It amounts to selecting a group of important $\mathbf{x}_i$ depending on a distance to another group of considered to be important points $\mathbf{x}_i$ that belong to the other class. The form of this distance function is often inspired by computational ease: some kind of kernel, for instance: $K(\mathbf{x}_i, \mathbf{x}_j) = \exp (\gamma || \mathbf{x}_i - \mathbf{x}_j ||^2 )$ which can be calculated by a simple dot product.

But is there a different approach? Certainly. As soon as we realize that we already put prior knowledge in our problem with respect to the form of the noise $\epsilon$, we might as well encode our search for "simple functions" in the form of another prior. Let us assume that $\mathbf{w}$ is generated by a zero-mean Gaussian distribution as well!

$$
p(\mathbf{w}|\mathbf{\alpha}) = \prod_{i=0}^{N-1} \mathcal{N}(w_i|0,\alpha_i^{-1})
$$

Suddenly we have another set of parameters, $\mathbf{\alpha}$ to take care of. And, shuddering, the next step of Bayesian-inclined person, is to introduce even more prior knowledge into this system. The parameters $\mathbf{\alpha}$ and the original noise variance $\sigma^2$ (or precision $\beta=-\sigma^2$) are considered so-called scale parameters and originate as well from a distribution:

$$
p(\mathbf{\alpha}) = \prod_{i=0}^{N-1} \text{Gamma}(\alpha_i|a,b) \\
p(\beta)=\text{Gamma}(\beta|c,d)
$$

Again, we see an additional level of hyperparameters $a,b,c$, and $d$. This time even a Bayesian person might stop with defining again new structures within this level of inference. Hereby it is assumed that all scales are equally likely. Note, that we already introduced some hierarchical levels nevertheless. An infinite hierarchy is not necessary with respect to interesting structure we might obtain from the data with our two levels of hyperparameters.

The hyperparameters can control **groups** of weights. The variable $\alpha_i$ might be much larger than $\alpha_j$ (and considering that $\alpha^{-1}$ is the inverse of the noise variable for $w_i$) its corresponding $p(w_i,\alpha_i)$ will be almost zero. So, the variable $\mathbf{x}_i$ will (almost) not be considered in $y(\mathbf{x}_i,\mathbf{w})$. It is deemed irrelevant! Somehow magically, by introducing additional levels of abstractions (or indirections) we arrive at a method with which we can ignore a large part of the dataset.



